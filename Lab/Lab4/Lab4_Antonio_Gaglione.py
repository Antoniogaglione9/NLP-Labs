# -*- coding: utf-8 -*-
"""Copia_di_Assignment (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_tmyHiZ_geqi_-8Qf3sPWo5GJcBj8ITf

## Imports

Here import all crucial packages etc.
"""

# Code here

import json
import os
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)
from sklearn.metrics import f1_score, precision_score, recall_score
#from torch.utils.data import Dataset
#from torch.utils.data import DataLoader
import torch
from transformers import EvalPrediction, pipeline

"""## Utils

Helper functions that you will use
"""

#Code here

os.environ["WANDB_DISABLED"] = "true"

class DisinformationDataset(torch.utils.data.Dataset):
    """
    This class wraps our tokenized data and labels so PyTorch can easily loop through them during training. It converts each input into tensors and returns them with the label — all in the format the model expects.
    """
    # When we create an instance of dataset, we pass in encodings and labels
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    # This method tells PyTorch how to get one item (input + label).
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    # Returns how many examples are in the dataset (needed by DataLoader).
    def __len__(self):
        return len(self.labels)


def load_and_process_data(file_path: str, label_column: str = "label") -> pd.DataFrame:
    """
    Loads the data from a CSV file and processes the labels.
    Args:
        file_path (str): Path to the CSV file.
        label_column (str): The column name containing the labels.
        text_column (str): The column name containing the text content.
    Returns:
        pd.DataFrame: Processed dataframe with labels and text content.
    """
    data = pd.read_csv(file_path, encoding='utf-8')
    data[label_column] = data[label_column].apply(lambda x: 1 if "fake" in x.lower() else 0)
    return data


def save_metrics_to_json(metrics: dict, output_file_path: str):
    """
    Saves the metrics to a JSON file.
    Args:
        metrics (dict): The evaluation metrics.
        output_file_path (str): The file path to save the metrics.
    """
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
    with open(output_file_path, 'w') as output_file:
        json.dump(metrics, output_file, indent=4)

def compute_metrics(pred=None, y_true=None, y_pred=None):
    """
    Computes F1 scores (micro, macro, weighted) for both training and testing data.

    If `pred` is provided, it computes metrics for the trainer using `EvalPrediction`.
    If `y_true` and `y_pred` are provided, it computes metrics for test data predictions.

    Parameters:
        - pred (EvalPrediction, optional): The evaluation prediction object for Trainer.
        - y_true (list, optional): The ground truth labels for the test data.
        - y_pred (list, optional): The predicted labels for the test data.

    Returns:
        - dict: A dictionary containing F1 metrics.
    """
    if pred is not None:
        # When working with the Trainer, pred is an EvalPrediction object
        labels = pred.label_ids
        y_pred = pred.predictions.argmax(-1)
    elif y_true is not None and y_pred is not None:
        # If y_true and y_pred are provided, use them for test evaluation
        labels = y_true
    else:
        raise ValueError("Either `pred` or both `y_true` and `y_pred` must be provided.")

        # Compute F1 scores
    f1 = f1_score(y_true=labels, y_pred=y_pred)
    precision = precision_score(y_true=labels, y_pred=y_pred)
    recall = recall_score(y_true=labels, y_pred=y_pred)

    return {
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def compute_metrics_for_trainer(pred: EvalPrediction):
    return compute_metrics(pred=pred)

"""# Assignment

# Fine-Tuning BERT Model to Fake News detection

## Import Train, Validation and Test data

Import all datasets and load and preprocess train and validation

Link to direcotry with data: https://github.com/ArkadiusDS/NLP-Labs/tree/master/data/CoAID/
"""

url_test = "https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/test.csv"
url_train = "https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/train.csv"
url_val = "https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/validation.csv"

!wget -O test.csv {url_test}
!wget -O train.csv {url_train}
!wget -O validation.csv {url_val}

train_data = load_and_process_data('train.csv')

validation_data = load_and_process_data('validation.csv')

"""## Load model and tokenizer

Firstly create two dicts id2label and label2id and then load model and tokenizer
Then use well-known distilled version of BERT model for faster fine-tuning: 'distilbert/distilbert-base-uncased' or any other model you wish.
"""

id2label = {0: "Credible", 1: "Fake"}
label2id = {"Credible": 0, "Fake": 1}

model0 = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)
model1 = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)
model2 = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)


tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(
        train_data['content'].tolist(),
        truncation=True,
        padding=True,
        max_length=130
    )

# Tokenizing the validation dataset
val_encodings = tokenizer(
        validation_data['content'].tolist(),
        truncation=True,
        padding=True,
        max_length=130
    )

"""## Tokenize datasets and prepare it for fine-tuning

You may use DisinformationDataset class for data preparation.
"""

train_dataset = DisinformationDataset(train_encodings, train_data['label'].tolist())

val_dataset = DisinformationDataset(val_encodings, validation_data['label'].tolist())

training_args0 = TrainingArguments(
    output_dir='output/training/',
    eval_strategy='steps',
    learning_rate=0.00001,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    warmup_ratio=0.06,
    weight_decay=0.1,
    fp16=True,
    metric_for_best_model='f1',
    load_best_model_at_end=True,
    save_total_limit=2,
    greater_is_better=True,
    save_strategy='steps',
    eval_steps=100,
    save_on_each_node=True,
    report_to=[]
)

training_args1 = TrainingArguments(
    output_dir='output/training/',
    eval_strategy='steps',
    learning_rate=0.00003,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    warmup_ratio=0.06,
    weight_decay=0.05,
    fp16=True,
    metric_for_best_model='f1',
    load_best_model_at_end=True,
    save_total_limit=2,
    greater_is_better=True,
    save_strategy='steps',
    eval_steps=100,
    save_on_each_node=True,
    report_to=[]
)

training_args2 = TrainingArguments(
    output_dir='output/training/',
    eval_strategy='steps',
    learning_rate=0.00005,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    warmup_ratio=0.1,
    weight_decay=0.02,
    fp16=True,
    metric_for_best_model='f1',
    load_best_model_at_end=True,
    save_total_limit=2,
    greater_is_better=True,
    save_strategy='steps',
    eval_steps=100,
    save_on_each_node=True,
    report_to=[]
)

trainer0 = Trainer(
        model=model0,  # Pass the actual model instance
        args=training_args0,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics_for_trainer
    )

trainer1 = Trainer(
        model=model1,  # Pass the actual model instance
        args=training_args1,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics_for_trainer
    )

trainer2 = Trainer(
        model=model2,  # Pass the actual model instance
        args=training_args2,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics_for_trainer
    )

"""## Fine-tune BERT model on at least 3 sets of hyperparameters

Check F1 score, precision and recall for each fine-tuned model and at the end choose set of hyperparameters that gives you best results. For each set of hyperparameters write down the final metrics. You need to acheive at least below result on validation dataset:

"f1": 0.91,
"recall": 0.91,
"precision": 0.91

Remember you need to achieve these minimum results on VALIDATION dataset and the best model on validation dataset will have to be used for predictions on test dataset.

"""

trainer0.train()

trainer1.train()

trainer2.train()

metrics0 = compute_metrics(pred=trainer0.predict(val_dataset))
metrics1 = compute_metrics(pred=trainer1.predict(val_dataset))
metrics2 = compute_metrics(pred=trainer2.predict(val_dataset))

trainer0.save_model("output/exp0/")
trainer1.save_model("output/exp1/")
trainer2.save_model("output/exp2/")

tokenizer.save_pretrained("output/exp0/")
tokenizer.save_pretrained("output/exp1/")
tokenizer.save_pretrained("output/exp2/")

best_exp_index = max(
    enumerate([metrics0, metrics1, metrics2]),
    key=lambda x: x[1]['f1']
)[0]

print(f"\n✅ Best model: experiment_{best_exp_index} selected based on highest F1 score ({[metrics0, metrics1, metrics2][best_exp_index]['f1']:.4f})")

"""## Final prediction on test dataset

Take best model and hyperparameters on validation and predict on test dataset. Compute evaluation metrics f1, precision and recall.
"""

test_data = load_and_process_data("test.csv")
test_encodings = tokenizer(
    test_data["content"].tolist(),
    truncation=True,
    padding=True,
    max_length=130
)
test_dataset = DisinformationDataset(test_encodings, test_data["label"].tolist())

best_model = [model0, model1, model2][best_exp_index]
trainer = Trainer(model=best_model)
test_metrics = compute_metrics(pred=trainer.predict(test_dataset))

print("=== Test metrics ===")
print(test_metrics)

"""# Final file with results and description"""

import json

"""All keys in your dictionary have to be the same as below. The only changes you should do in terms of keys is changing names of hyperparameters, e.g. instead of key "name_of_hyperparameter_0" if you used learning rate then write "learning_rate". Other important information in the dictionary below and comments. Each value says what is expected.

Example dictionary provided under the template.

Template for your structured resulting file
"""

data = {
    # Everything in experiment_0 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.
    "experiment_0": {
        "model": "distilbert-base-uncased",
        "hyperparameters": {
            "Learning rate": "0.00001",
            "Warmup ratio": "0.06",
            "Weight decay": "0.1"
        },
        "f1_score": "0.9476",
        "precision": "0.9694",
        "recall": "0.9268",
        "description": "Experiment 0 uses a conservative learning rate and moderate weight decay to ensure stability and avoid overfitting. This configuration prioritizes gradual learning and robustness."
    },
    # Everything in experiment_1 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.
    "experiment_1": {
        "model": "distilbert-base-uncased",
        "hyperparameters": {
            "Learning rate": "0.00003",
            "Warmup ratio": "0.06",
            "Weight decay": "0.5"

        },
        "f1_score": "0.9550",
        "precision": "0.9795",
        "recall": "0.9317",
        "description": "Experiment 1 increases the learning rate for faster convergence and applies a high weight decay to strongly regularize the model. This setup aims to generalize well by avoiding overfitting."
    },
    # Everything in experiment_2 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.
    "experiment_2": {
        "model": "distilbert-base-uncased",
        "hyperparameters": {
            "Learning rate": "0.00005",
            "Warmup ratio": "0.1",
            "Weight decay": "0.02"
        },
        "f1_score": "0.9726",
        "precision": "0.9949",
        "recall": "0.9512",
        "description": "Experiment 2 adopts a more aggressive learning strategy, combining a high learning rate with minimal regularization and longer warmup. This allows the model to adapt quickly and thoroughly to the training data."
    },
    # Everything in final_prediction is related to prediction on test dataset, so metrics are computed on test dataset etc.
    "final_prediction": {
        "model": "distilbert-base-uncased",
        "experiment_chosen": "experiment_0 or experiment_1 or experiment_2",
        "hyperparameters": {
            "Learning rate": "0.00005",
            "Warmup ratio": "0.1",
            "Weight decay": "0.02"
        },
        "f1_score": "0.9854",
        "precision": "0.9902",
        "recall": "0.9806",
        "description": "Final prediction is based on experiment 2, which showed the best performance on the validation set. The model achieved excellent results on the test set, demonstrating strong generalization and high reliability."
    }
}

with open("experiments_Antonio_Gaglione_2129563.json", "w") as f:
    json.dump(data, f, indent=4)

"""## Example final file"""

data = {
    "experiment_0": {
        "model": "google-bert/bert-base-uncased",
        "hyperparameters": {
            "learning_rate": "float",
            "warmap_ratio": "float",
            "weight_decay": "float"
        },
        "f1_score": "float",
        "precision": "float",
        "recall": "float",
        "description": "This experiment fine-tuned the google-bert/bert-base-uncased model for binary classification using a learning rate of 1e-5 and a warmup ratio of 0.06. The model achieved an F1-score of 0.76, with a strong recall of 0.85, indicating high sensitivity to positive cases. Precision was moderate at 0.65, suggesting some trade-off in false positives. The setup demonstrates effective recall-oriented performance in identifying relevant instances."
    },
    "experiment_1": {
        "model": "google-bert/bert-base-uncased",
        "hyperparameters": {
            "learning_rate": "float",
            "weight_decay": "float"
        },
        "f1_score": "float",
        "precision": "float",
        "recall": "float",
        "description": "Unique description two of the approach - it has to be different for each experiment. Everything in experiment_1 is related to experiment on validation dataset, so metrics are computed on validation dataset etc."
    },
    "experiment_2": {
        "model": "google-bert/bert-base-uncased",
        "hyperparameters": {
            "learning_rate": "float",
            "num_train_epochs": "int",
            "weight_decay": "float"
        },
        "f1_score": "float",
        "precision": "float",
        "recall": "float",
        "description": "Unique description three of the approach - it has to be different for each experiment. Everything in experiment_2 is related to experiment on validation dataset, so metrics are computed on validation dataset etc."
    },
    "final_prediction": {
        "model": "google-bert/bert-base-uncased",
        "experiment_chosen": "experiment_0",
        "hyperparameters": {
            "learning_rate": "float",
            "warmap_ratio": "float"
        },
        "f1_score": "float",
        "precision": "float",
        "recall": "float",
        "description": "Unique description four of the final results and prediction - it has to be different and here you will describe results on test dataset. Everything in final_prediction is related to prediction on test dataset, so metrics are computed on test dataset etc."
    }
}

with open("experiments_Arkadiusz_Modzelewski_29580.json", "w") as f:
    json.dump(data, f, indent=4)

with open("experiments_Arkadiusz_Modz...", "r") as f:
    print(f.read())