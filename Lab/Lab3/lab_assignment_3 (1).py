# -*- coding: utf-8 -*-
"""Lab assignment 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pb9mEzNvi12KWVI-OoO294ATCRH2cKBJ
"""

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import math
import pandas as pd

!git clone https://github.com/elenipapadopulos/NLP_LAB3_Datasets.git

df = pd.read_csv("/content/NLP_LAB3_Datasets/typo_dataset1.csv")

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)


bos_token = "<s>"
tokenizer.add_tokens([bos_token])
tokenizer.bos_token = bos_token
model.resize_token_embeddings(len(tokenizer))

threshold = 11.5

def get_token_logprobs(sentence):

    tokenizer.pad_token = tokenizer.eos_token

    model.config.pad_token_id = tokenizer.pad_token_id

    batch = tokenizer(sentence, return_tensors="pt").to(device)

    output = model(**batch)

    logits = output.logits.squeeze(0)

    logits = logits[:-1, :]

    input_ids = batch["input_ids"].squeeze(0)[1:]

    log_probs = torch.log_softmax(logits, dim=-1)

    token_logprobs = log_probs[range(len(input_ids)), input_ids]

    tokens = tokenizer.convert_ids_to_tokens(batch["input_ids"].squeeze(0))

    return tokens[1:], token_logprobs.tolist()

def get_cumulative_token_logprobs(sentence):
    tokens, token_logprobs = get_token_logprobs(sentence)

    cumulative_logprobs = []
    cumulative_prob = 0.0

    for token, log_prob in zip(tokens, token_logprobs):
        cumulative_prob += log_prob
        cumulative_logprobs.append((token, cumulative_prob))

    return cumulative_logprobs


def detect_typos(sentence, threshold=5):

    word_logprobs = get_cumulative_token_logprobs(sentence)

    for i in range(1, len(word_logprobs)):

        logprob_diff = abs(word_logprobs[i][1] - word_logprobs[i-1][1])

        if logprob_diff > threshold:
            return 0

    return 1



def evaluate_threshold(df, threshold):
    correct = 0
    for i, row in df.iterrows():
        sentence, label = row["text"], row["label"]
        pred = detect_typos(sentence, threshold)
        if label == pred:
            correct += 1
    accuracy = correct / len(df)
    return accuracy


accuracy = evaluate_threshold(df, threshold)
print(f"Threshold: {threshold}, Accuracy: {accuracy}")