# -*- coding: utf-8 -*-
"""Antonio Gaglione Lab-1-Text-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kV7lbvz_AMKW86rbuWKedP8FpwTTMBXK

#Lab 1: Introduction to ScikitLearn and Classification Tasks

During this Lab, we aim to achieve the following:


*   Familiarize with <a href="https://scikit-learn.org/stable/"> scikit-learn </a>, an essential python library in data science;
*   learn how to approach a classification task with scikit-learn.

In this notebook, we learn to use Scikit-Learn with a practical example and then, in the second part, we will test our knowledge by doing some exercises.

# Part 1: A Classification Example With Scikit-Learn

We start our lab by implementing *Logistic Regression* using  scikit-learn.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

from collections import Counter

X_toy, y_toy = datasets.make_blobs(n_samples=150,n_features=2,
                           centers=2,cluster_std=3.05,
                           random_state=2) # create an artificial dataset
print(X_toy)
print(y_toy)

print(Counter(y_toy))

#Plotting the data
fig = plt.figure(figsize=(10,8))
plt.plot(X_toy[:, 0][y_toy == 0], X_toy[:, 1][y_toy == 0], 'r^')
plt.plot(X_toy[:, 0][y_toy == 1], X_toy[:, 1][y_toy == 1], 'gs')
plt.xlabel("X1")
plt.ylabel("X2")
plt.title('Random Classification Data with 2 classes')
plt.show()

"""We can now define our classifier: logistic regression <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"> [link] </a>."""

from sklearn.linear_model import LogisticRegression

clf_lr = LogisticRegression() # options for the classifiers are passed as parameters to constructor of the class
                              # LogisticRegression(). Either visit the link or put the cursor over it to see them

"""Sklearn defines standard functions for models, like *fit* and *predict*."""

#train phase
clf_lr.fit(X_toy, y_toy)


#estimation (y_hat)
y_pred_cl_lr = clf_lr.predict(X_toy)

print(y_pred_cl_lr)

"""How to evaluate our models' performance? <br>
Scikit-learn offers a broad set of evaluation functions already implemented <a href = "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics">[link]</a>.
"""

from sklearn.metrics import accuracy_score

print(f"Logistic Regression -- Toy dataset.\tACC: {accuracy_score(y_toy, y_pred_cl_lr)}")

# printing the values of the parameters after learning

print(clf_lr.coef_[0])

"""## Model Selection
When defining or training a model, we have the so called *hyperparameters*, i.e., different settings to configure for our training strategy.  <br>
The question is: *how can we decide the best configuration setting for the task?* <br>
The answer is the usage of *training* and *validation* partitions. <br>
We can use sklearn to do that: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html.
"""

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X_toy, y_toy,
                                                  train_size = 0.8, shuffle=False, random_state=42)

print(f"Original size = {X_toy.shape[0]}\tTrain size = {X_train.shape[0]}\tVal size = {X_val.shape[0]}")  # alternative way to use the print when there are
                                                                                                          # variables and text to print together

"""First we use Scikit-Learn to train a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression</a> classifier with default parameters over the Toy dataset. <br>
We compute the accuracy on both training and validation sets.

"""

from sklearn.linear_model import LogisticRegression

clf_lr = LogisticRegression()
clf_lr.fit(X_train, y_train)

#estimation (y_hat)
y_train_pred_lr = clf_lr.predict(X_train)
y_val_pred_lr = clf_lr.predict(X_val)

print(f"Logistic Regression.\tTrain ACC: {accuracy_score(y_train, y_train_pred_lr)}")
print(f"Logistic Regression.\tVal ACC: {accuracy_score(y_val, y_val_pred_lr)}")

"""### Model Selection with Logistic Regression

Logistic Regression has a hyperparameter *C*. Lower values of C correspond to simpler models (with the risk of underfitting), higher values of C correspond to complex models (with the risk of overfitting).  
Let's see how the performance change by varying it.

Let's find the best *C* among the following: $C = [0.001, 0.01, 0.1, 1., 10]$.
"""

C = [0.001, 0.01, 0.1, 1., 10, 100]
for c in C:
    clf_lr = LogisticRegression(C = c)
    clf_lr.fit(X_train, y_train)

    #estimation (y_hat)
    y_train_pred_lr = clf_lr.predict(X_train)
    y_val_pred_lr = clf_lr.predict(X_val)
    tr_acc = accuracy_score(y_train, y_train_pred_lr)
    val_acc= accuracy_score(y_val, y_val_pred_lr)

    print(f"LR. C= {c}.\tTrain ACC: {tr_acc}\tVal Acc: {val_acc}")

"""### Exercise: Model Selection with Logistic Regression

We ask you again to work on a classification task. <br>
This time, the classification is more challenging.
The dataset is called *sonar*.
"""

import pandas as pd
import random
random.seed(42)

url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'
ds = pd.read_csv(url, header = None)

# split into input and output elements
data = ds.values
random.shuffle(data)
X_sonar, y_sonar = data[:, :-1], data[:, -1]

print(X_sonar.shape, y_sonar.shape)

"""It's time to partition our dataset. <br>
We ask you to create three partitions:


*   *train set* : a set of samples used to train a model.
*   *val set*: a set of samples used to decide the best model.
*   *test set*: a set of samples used to see our best model performance.

We now first split samples that we can use in our training (train and val), from samples that we cannot touch (test). <br>
**EX 1** Create a split between train_val and test, by maintaining the 25% of samples in the test set.
"""

#
# Ex 1: complete here
from sklearn.model_selection import train_test_split
X_train_val, X_test, y_train_val, y_test = train_test_split(X_sonar, y_sonar, test_size=0.25, random_state=42)

"""**EX 1.2** From the train_val variables, split train and validation sets. Maintain the 10% of samples in the validation.

"""

#
# Ex 1.2: complete here
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)

print(X_train.shape, X_val.shape, X_test.shape)

"""**EX 2** Train and evaluate (using accuracy) a logistic regression with the default value for the hyperparameter. Do the evaluation **only** on the training and validation partitions."""

#
# Ex 2: complete here
from sklearn.linear_model import LogisticRegression
modello=LogisticRegression()
modello.fit(X_train, y_train)
y_train_pred=modello.predict(X_train)
y_val_pred=modello.predict(X_val)
from sklearn.metrics import accuracy_score
print(f"Logistic Regression. /tTrain ACC: {accuracy_score(y_train, y_train_pred)}")
print(f"Logistic Regression. /tVal ACC: {accuracy_score(y_val, y_val_pred)}")

"""Let's find the best value for *C*, an hyperparameter of the model. <br>
See the documentation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"> [link] </a>. <br>
**EX 3**  We ask you to find the best *C* among the following: $C = [0.001, 0.01, 0.1, 1., 10, 100, 1000, 10000]$.
"""

#
# Ex 3: complete here
#
C = [0.001, 0.01, 0.1, 1., 10, 100, 1000, 10000]
for c in C:
    hyper_model = LogisticRegression(C = c)
    hyper_model.fit(X_train,y_train)

    y_hyper = hyper_model.predict(X_train)
    y_hyper_val = hyper_model.predict(X_val)
    hyper_acc_train = accuracy_score(y_train, y_hyper)
    hyper_acc_val = accuracy_score(y_val, y_hyper_val)
    print(f"Logistic Regression (C={c})\tTrain ACC: {hyper_acc_train}")
    print(f"Logistic Regression. /tVal ACC: {hyper_acc_val}")

"""**Ex 4** It's time to see the performance on the test set of the best model, after training it on the training set. Use the accuracy as evaluation metric."""

#
# Ex 4: complete here
hyper_model.fit(X_train, y_train)
y_test_pred = hyper_model.predict(X_test)
acc_test = accuracy_score(y_test, y_test_pred)
print(f" L'accuracy sul test set Ã¨: {acc_test}")

"""## Computing Vectorial Representations"""

!python -m spacy download "en_core_web_sm"
import spacy

from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]


vectorizer = CountVectorizer()
vectorizer.fit(corpus)
X = vectorizer.transform(corpus)

print(vectorizer.get_feature_names_out())
print("vocabulary size:", len(vectorizer.get_feature_names_out()))

# Let's see some of the options of CountVectorizer (first we change the tokenizer)
nlp_en = spacy.load("en_core_web_sm", disable=['ner', 'parser'])

def spacy_tokenizer(text):
  return [token.text for token in nlp_en(text)]

vectorizer2 = CountVectorizer(binary=False, tokenizer=spacy_tokenizer)
X=vectorizer2.fit_transform(corpus)
print(vectorizer2.get_feature_names_out())
print("vocabulary size:", len(vectorizer2.get_feature_names_out()))

# now I can use X and Y in a learning algorithm

"""#Sentiment Analysis

The dataset is described here: https://www.aclweb.org/anthology/P04-1035.pdf

It is part of nltk, so it is convenient for us to use.

The goal of this exercise is to build a first machine learning model using the tools that we have seen so far: choose how to preprocess the text, create a bag of words feature representation, train a model using an ML method of your choice.

You need to use the following split for the data:

*   test: 30% of the documents
*   The rest of the documents will be split as
    *   train: 75% of the documents
    *   validation: 30% of the documents


Use accuracy as evaluation measure.
"""

import nltk
nltk.download('movie_reviews') # loads the dataset
nltk.download('punkt')
#!python -m spacy download "en_core_web_sm"

"""## Loading the data
In the following I extract the raw content of the reviews (movie_reviews.raw()), i.e. each review is a string.
Another option is to use movie_reviews.words() that returns each review as a list of tokens. Feel free to use whichever best fit your needs.

"""

from nltk.corpus import movie_reviews
import random
import spacy
from scipy.sparse import coo_matrix, vstack
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

nlp_en = spacy.load("en_core_web_sm", disable=['ner', 'parser'])

documents = [(movie_reviews.raw(fileid), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

print("number of docs loaded:", len(documents))

corpus_raw = [ x[0] for x in documents ]
y_corpus = [ x[1] for x in documents ]
print(corpus_raw[0])
print(y_corpus[0])

random.seed(42)

"""## Exercise

Create a vectorial representation of the data, then apply a learning algorithm by optimising the hyperparameters on the dev set. If you need to use any function that depends on random number generators, use 42 as seed.
Test several representations. You may try functions of the libraries we have seen in class or make your own vectorial representation from scratch.
Once you have selected the best hyperparameters and preprocessing, retrain your model on the union of the training and validation sets, then compute the accuracy on the test set.

Report your test performance on Moodle. In Moodle you are also supposed to upload the notebook in .py format (Menu File->Download->Download .py)
In the file with your code motivate any significant choice you made and all different preprocessing you attempted (clearly highlight the best one, though).

**Bonus Exercise** for your best model, print the 30 tokens whose corresponding parameter have highest absolute value. What do you think of this list? Does it make sense? Are all tokens expected?

# **The best score i obtained on the test set, in terms of accuracy, was 0.857. I attached 3 different codes, expalining the steps that brought me to produce these results.**

Before obtaining that i just tried to apply what we've seen in class and during the lab, splitting the dataset in training, validation and test, shuffling the data since they were divided by label. I then used CountVectorizer to create a vectorial representation of the data, i also encoded the label using LabelEncoder. Then just using the given model(LogisticRegression) i trained on the trainset and evaluated on the valset, this for different values of C. Finally evaluating on the test set with a value of C=10, obtaining an accuracy of 0.823
"""

import nltk
nltk.download('movie_reviews') # loads the dataset
nltk.download('punkt')
#!python -m spacy download "en_core_web_sm"
from nltk.corpus import movie_reviews
import random
import spacy
from scipy.sparse import coo_matrix, vstack
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

nlp_en = spacy.load("en_core_web_sm", disable=['ner', 'parser'])

documents = [(movie_reviews.raw(fileid), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

print("number of docs loaded:", len(documents))

corpus_raw = [ x[0] for x in documents ]
y_corpus = [ x[1] for x in documents ]

random.seed(42)
from sklearn.model_selection import train_test_split
X_train_val, X_test, y_train_val, y_test = train_test_split(corpus_raw, y_corpus, train_size = 0.7, shuffle=True, random_state=42)
print(len(X_train_val),len(X_test))

y2_train_val = y_train_val.copy()

X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.75, shuffle=False, random_state=42)
print(len(X_train), len(X_val))

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(X_train)
train = vectorizer.transform(X_train)
train_val = vectorizer.transform(X_train_val)
val = vectorizer.transform(X_val)
test = vectorizer.transform(X_test)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_val = encoder.transform(y_val)
y_test = encoder.transform(y_test)
y_train_and_val = encoder.transform(y2_train_val)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

C = [0.001, 0.01, 0.1, 1., 10, 100, 1000]
for c in C:
  model = LogisticRegression(C = c)
  model.fit(train, y_train)

  y_train_pred = model.predict(train)
  y_val_pred = model.predict(val)

  train_acc= accuracy_score(y_train, y_train_pred)
  val_acc = accuracy_score(y_val, y_val_pred)

  print(f"LR. C= {c}.\tTrain ACC: {train_acc}\tVal Acc: {val_acc}")

model_test= LogisticRegression(C = 10)
model_test.fit(train_val, y_train_and_val)
y_test_pred = model_test.predict(test)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy(C=10) : {test_acc}")

"""After the first attempt, i noticed that for lower values of C the predefined number of iteration of our model didn't permit to reach convergence, to solve that i modifed the limit value to 2500. To improve the quality of the vocabulary i used stop_word='english' in order to rid off all of that terms that are not usefull for the sentiment analysis. The last change i made was using saga as solver, that should be better for large and sparce dataset as the nlp ones. All of that translated on a slight improvement on the performance, obtaining 0.837 as accuracy score, using C=100."""

import nltk
nltk.download('movie_reviews') # loads the dataset
nltk.download('punkt')
#!python -m spacy download "en_core_web_sm"

from nltk.corpus import movie_reviews
import random
import spacy
from scipy.sparse import coo_matrix, vstack
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

nlp_en = spacy.load("en_core_web_sm", disable=['ner', 'parser'])

documents = [(movie_reviews.raw(fileid), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

print("number of docs loaded:", len(documents))

corpus_raw = [ x[0] for x in documents ]
y_corpus = [ x[1] for x in documents ]

random.seed(42)

from sklearn.model_selection import train_test_split
X_train_val, X_test, y_train_val, y_test = train_test_split(corpus_raw, y_corpus, train_size = 0.7, shuffle=True, random_state=42)
print(len(X_train_val),len(X_test))

y2_train_val = y_train_val.copy()

X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.75, shuffle=False, random_state=42)
print(len(X_train), len(X_val))

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(stop_words="english")
vectorizer.fit(X_train)
train = vectorizer.transform(X_train)
train_val = vectorizer.transform(X_train_val)
val = vectorizer.transform(X_val)
test = vectorizer.transform(X_test)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_val = encoder.transform(y_val)
y_test = encoder.transform(y_test)
y_train_and_val = encoder.transform(y2_train_val)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

C = [0.001, 0.01, 0.1, 1., 10, 100, 1000]
for c in C:
  model = LogisticRegression(C=c, max_iter=2500, solver='saga')
  model.fit(train, y_train)

  y_train_pred = model.predict(train)
  y_val_pred = model.predict(val)

  train_acc= accuracy_score(y_train, y_train_pred)
  val_acc = accuracy_score(y_val, y_val_pred)

  print(f"LR. C= {c}.\tTrain ACC: {train_acc}\tVal Acc: {val_acc}")

model_test= LogisticRegression(C = 100)
model_test.fit(train_val, y_train_and_val)
y_test_pred = model_test.predict(test)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_acc}")

"""As last attempt i tired preprocessing the text using nltk. In particular using the function preprocess, where i apply tokenziation and lammatization to the dataset. This produce an higher quality vocabulary,first converting the words in lowercase,then throughout lemmatization we can also reduce the vocabulary size,and at last ridding off everything else than words. At last i changed the vectorial representation of the data using tf_idf vectorizer, in particular applying a log transformation (sublinear_tf =true), then i used also ngram=2 to consider coupple of words instead of only single words, this makes the process heavier but also more meaningfull. Then i excluded from the vocabularthe most frequent and the last frequent words. At the end i obtained a test accuracy, with C=1000, of 0.856


"""

import nltk
nltk.download('movie_reviews') # loads the dataset
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
#!python -m spacy download "en_core_web_sm"

from nltk.corpus import movie_reviews
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import random
import spacy
from scipy.sparse import coo_matrix, vstack
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

nlp_en = spacy.load("en_core_web_sm", disable=['ner', 'parser'])

documents = [(movie_reviews.raw(fileid), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

print("number of docs loaded:", len(documents))
corpus_raw = [ x[0] for x in documents ]
y_corpus = [ x[1] for x in documents ]

random.seed(42)

lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha()]
    return " ".join(tokens)

corpus_clean = [preprocess(text) for text in corpus_raw]

from sklearn.model_selection import train_test_split
X_train_val, X_test, y_train_val, y_test = train_test_split(corpus_clean, y_corpus, train_size = 0.7, shuffle=True, random_state=42)

y2_train_val = y_train_val.copy()

X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.75, shuffle=False, random_state=42)

vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,2), min_df=5, max_df=0.9)
vectorizer.fit(X_train)
train = vectorizer.transform(X_train)
train_val = vectorizer.transform(X_train_val)
val = vectorizer.transform(X_val)
test = vectorizer.transform(X_test)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_val = encoder.transform(y_val)
y_test = encoder.transform(y_test)
y_train_and_val = encoder.transform(y2_train_val)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

C = [0.001, 0.01, 0.1, 1., 10, 100, 1000]
for c in C:
  model = LogisticRegression(C=c, max_iter=2500, solver='saga')
  model.fit(train, y_train)

  y_train_pred = model.predict(train)
  y_val_pred = model.predict(val)

  train_acc= accuracy_score(y_train, y_train_pred)
  val_acc = accuracy_score(y_val, y_val_pred)

  print(f"LR. C= {c}.\tTrain ACC: {train_acc}\tVal Acc: {val_acc}")

model_test= LogisticRegression(C = 1000)
model_test.fit(train_val, y_train_and_val)
y_test_pred = model_test.predict(test)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy(C=1000): {test_acc}")

"""Bonus exercise: Printing the most valuable tokens we can observe that almost all of them are words really meaningfull for sentiment analisys, as for example wasted, fun , the worst, ecc..."""

import numpy as np

coefficients = model_test.coef_.flatten()

tokens = vectorizer.get_feature_names_out()

top_30_indices = np.argsort(np.abs(coefficients))[-30:]

top_30_tokens = [tokens[i] for i in top_30_indices]

print("Top 30 tokens with the highest absolute coefficients:")
for token in top_30_tokens:
    print(token)